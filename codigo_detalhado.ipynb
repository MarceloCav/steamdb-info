{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping e Integração com BigQuery\n",
    "Este notebook visa apenas detalhar o código ```steam_db_sales_scraper.py``` de entrega continua do web scraping no site SteamDB para coletar dados de vendas de jogos no Steam e envia esses dados para o Google BigQuery.\n",
    "\n",
    "O processo é dividido em várias etapas:\n",
    "- **Atualização de Cookies**: Coleta de cookies necessários para burlar o Cloudflare.\n",
    "- **Coleta de Dados**: Scrape das informações de vendas de jogos no Steam.\n",
    "- **Armazenamento no BigQuery**: Envio dos dados coletados para o Google BigQuery.\n",
    "\n",
    "Vamos começar com a instalação das bibliotecas necessárias e a configuração do ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb6a5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar bibliotecas necessárias (se ainda não estiverem instaladas)\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Carregar as variáveis de ambiente\n",
    "Primeiro, carregamos as variáveis de ambiente usando o arquivo `.env` que contém as chaves de acesso à API e credenciais do Google Cloud.\n",
    "\n",
    "O uso do scrapedo foi apenas para ter proxies rotativos para cada requisição evitando o bloqueio por IP do CloudFlare, poderiamos usar outras ferramentas de rotatividade de ip ou proxie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31cc9f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_APPLICATION_CREDENTIALS = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')\n",
    "PROJECT_ID = os.getenv('PROJECT_ID')\n",
    "DATASET_ID = os.getenv('DATASET_ID')\n",
    "TABLE_ID = os.getenv('TABLE_ID')\n",
    "SCRAPEDO_API_KEY = os.getenv('SCRAPEDO_API_KEY')\n",
    "\n",
    "print(f'GOOGLE_APPLICATION_CREDENTIALS: {GOOGLE_APPLICATION_CREDENTIALS}')\n",
    "print(f'PROJECT_ID: {PROJECT_ID}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Função para Atualizar os Cookies\n",
    "A função `atualizar_cookies` é responsável por obter os cookies necessários para contornar a proteção do Cloudflare e permitir o acesso ao site SteamDB. Ela utiliza o módulo `CloudflareBypasser` para contornar a verificação de segurança.\n",
    "\n",
    "O site, por ser protegido por CloudFlare, as requisicoes sao validadas a partir de 2 cookies `cf_bm` e `cf_Clearence`, então eu forço o cloudflare no site da steam ao acessar `/cloudflare`, uso o bypass e consigo os cookies para validar as requisições.\n",
    "\n",
    "Seria mais interessante tornar o ByPass como um serviço para ser chamado e retornar o cookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def-cookies",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from CloudflareBypasser import CloudflareBypasser\n",
    "from DrissionPage import ChromiumPage, ChromiumOptions\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def atualizar_cookies(url=\"https://steamdb.info/cloudflare\"):\n",
    "    logging.info(\"Iniciando atualização de cookies.\")\n",
    "    tentativas = 0\n",
    "    max_tentativas = 5\n",
    "    cookies_necessarios = {}\n",
    "\n",
    "    while tentativas < max_tentativas:\n",
    "        logging.info(f\"Tentativa {tentativas + 1} de {max_tentativas}.\")\n",
    "        options = ChromiumOptions()\n",
    "        options.incognito(True)\n",
    "        driver = ChromiumPage(options)\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(random.randint(5, 10))\n",
    "            cf_bypasser = CloudflareBypasser(driver)\n",
    "            cf_bypasser.click_verification_button()\n",
    "            time.sleep(3)\n",
    "\n",
    "            cookies = driver.cookies()\n",
    "            logging.info(f\"Cookies coletados: {cookies}\")\n",
    "            for cookie in cookies:\n",
    "                if cookie['name'] == 'cf_clearance':\n",
    "                    cookies_necessarios['cf_clearance'] = cookie['value']\n",
    "                elif cookie['name'] == '__cf_bm':\n",
    "                    cookies_necessarios['__cf_bm'] = cookie['value']\n",
    "\n",
    "            if 'cf_clearance' in cookies_necessarios and '__cf_bm' in cookies_necessarios:\n",
    "                return cookies_necessarios\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Erro durante a tentativa de obter cookies: {e}\")\n",
    "\n",
    "        finally:\n",
    "            driver.close()\n",
    "\n",
    "        tentativas += 1\n",
    "        logging.warning(f\"Tentativa {tentativas}: Cookie cf_clearance não encontrado, tentando novamente...\")\n",
    "\n",
    "    logging.error(\"Não foi possível obter o cookie cf_clearance após 5 tentativas.\")\n",
    "    raise Exception(\"Não foi possível obter o cookie cf_clearance após 5 tentativas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Coleta de Dados de Vendas do Steam\n",
    "A função `obter_dados_steam_sales` realiza o scraping dos dados de vendas do SteamDB. Ela acessa o site SteamDB, coleta as informações de cada jogo, como nome, desconto, preço, classificação, entre outros, e organiza esses dados em um `DataFrame` do Pandas.\n",
    "\n",
    "Eu implementei uma função a mais que pegaria mais dados detalhados para cada jogo, mas é feita uma requisição por jogo, isso eu fiquei limitado pelo scrapedo, e eu precisaria de uma lista de proxies para gerar uma maior rotatividade de ips\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "def-dados-steam",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import re\n",
    "\n",
    "def obter_dados_steam_sales():\n",
    "    url = \"https://steamdb.info/sales/\"\n",
    "    headers = {\n",
    "        \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "        \"accept-language\": \"pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "        \"cache-control\": \"max-age=0\",\n",
    "        \"priority\": \"u=0, i\",\n",
    "        \"sec-ch-ua\": \"\\\"Google Chrome\\\";v=\\\"129\\\", \\\"Not=A?Brand\\\";v=\\\"8\\\", \\\"Chromium\\\";v=\\\"129\\\"\",\n",
    "        \"sec-ch-ua-arch\": \"\\\"x86\\\"\",\n",
    "        \"sec-ch-ua-bitness\": \"\\\"64\\\"\",\n",
    "        \"sec-ch-ua-full-version\": \"\\\"129.0.6668.100\\\"\",\n",
    "        \"sec-ch-ua-full-version-list\": \"\\\"Google Chrome\\\";v=\\\"129.0.6668.100\\\", \\\"Not=A?Brand\\\";v=\\\"8.0.0.0\\\", \\\"Chromium\\\";v=\\\"129.0.6668.100\\\"\",\n",
    "        \"sec-ch-ua-mobile\": \"?0\",\n",
    "        \"sec-ch-ua-model\": \"\\\"\\\"\",\n",
    "        \"sec-ch-ua-platform\": \"\\\"Linux\\\"\",\n",
    "        \"sec-ch-ua-platform-version\": \"\\\"6.8.0\\\"\",\n",
    "        \"sec-fetch-dest\": \"document\",\n",
    "        \"sec-fetch-mode\": \"navigate\",\n",
    "        \"sec-fetch-site\": \"none\",\n",
    "        \"sec-fetch-user\": \"?1\",\n",
    "        \"upgrade-insecure-requests\": \"1\"\n",
    "    }\n",
    "\n",
    "    # Realizando a requisição GET para obter os dados da página\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Criando um objeto BeautifulSoup para fazer o parse do HTML\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Buscando a tabela de jogos\n",
    "    jogos = []\n",
    "    tabela_jogos = soup.find(\"table\", class_=\"table-bordered\")\n",
    "    linhas = tabela_jogos.find_all(\"tr\")[1:]  # Pulando o cabeçalho da tabela\n",
    "\n",
    "    for linha in linhas:\n",
    "        colunas = linha.find_all(\"td\")\n",
    "        if len(colunas) > 6:\n",
    "            nome_jogo = colunas[1].get_text(strip=True)\n",
    "            desconto = colunas[2].get_text(strip=True)\n",
    "            preco_atual = colunas[3].get_text(strip=True)\n",
    "            preco_original = colunas[4].get_text(strip=True)\n",
    "            data_fim = colunas[5].get_text(strip=True)\n",
    "            link = colunas[1].find('a')['href']\n",
    "            jogos.append({\n",
    "                'nome_jogo': nome_jogo,\n",
    "                'desconto': desconto,\n",
    "                'preco_atual': preco_atual,\n",
    "                'preco_original': preco_original,\n",
    "                'data_fim': data_fim,\n",
    "                'link': link,\n",
    "                'data_coleta': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            })\n",
    "\n",
    "    # Convertendo para DataFrame\n",
    "    df = pd.DataFrame(jogos)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Envio dos Dados para o Google BigQuery\n",
    "Agora, vamos criar uma função que envia os dados coletados para o Google BigQuery, utilizando a biblioteca `google-cloud-bigquery`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bigquery-insertion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "def enviar_para_bigquery(df):\n",
    "    client = bigquery.Client.from_service_account_json(GOOGLE_APPLICATION_CREDENTIALS)\n",
    "    table_id = f'{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}'\n",
    "    try:\n",
    "        # Enviar os dados para o BigQuery\n",
    "        job = client.load_table_from_dataframe(df, table_id)\n",
    "        job.result()  # Aguardar o carregamento\n",
    "        logging.info(f'Dados enviados com sucesso para o BigQuery: {table_id}')\n",
    "    except Exception as e:\n",
    "        logging.error(f'Erro ao enviar dados para o BigQuery: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Comentários sobre minhas abordagens\n",
    "\n",
    "- Eu sempre tento deixar o código da forma mais escalável possível, quis evitar ao maximo uso de Selenium ou algo do tipo, por isso priorizei conseguir realizar requests\n",
    "\n",
    "- Ao perceber o CloudFlare no site, eu já tinha essa ferramenta de ByPass que funciona e consegui capturar os cookies do cloudflare. Como mencionado, eu tinha a ideia de tornar o ByPass como um servico separado, mesmo que ele não consiga ser feito no navegador em modo headless. Pelo timestamp que o cookie tem, eles duram 3 horas.\n",
    "\n",
    "- Decidi colocar todas as variaveis sensiveis com .env por motivos de segurança\n",
    "\n",
    "- Implementei diversos logs ao longo do processo de coleta, tratamento e ingestao dos dados, de forma que fique claro onde ocorreu algum erro. Logs esses que podem ser enviados como mensagem para um healthchecks ou slack para manutenção.\n",
    "\n",
    "- O código está pronto para rodar em um serviço continuo, como por exemplo, colocar num container Docker, ao separar o Bypass como servico, e rodar periodicamente atraves de cronjobs. \n",
    "\n",
    "- Fiz um pequeno dashboard em streamlit para conectar o banco de dados da google e implementei algumas analises dos dados.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
