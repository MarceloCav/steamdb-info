{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-07 03:50:25\n",
      "1731033153\n"
     ]
    }
   ],
   "source": [
    "#timestamp para utc e utf \n",
    "\n",
    "timestamp = 1730951425\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "utc = datetime.utcfromtimestamp(timestamp)\n",
    "\n",
    "print(utc)\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "timestamp = datetime.timestamp(now)\n",
    "\n",
    "print(int(timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<tr class=\"app\" data-appid=\"1086940\" data-capsule=\"capsule_231x87.jpg?t=1730915928\">\n",
    "\n",
    "<td data-sort=\"14\" class=\"dt-type-numeric\">\n",
    "<a target=\"_blank\" href=\"https://store.steampowered.com/app/1086940/?curator_clanid=4777282&amp;utm_source=SteamDB\" class=\"info-icon\" title=\"Store\"></a>\n",
    "</td>\n",
    "\n",
    "<td class=\"applogo\">\n",
    "<a target=\"_blank\" href=\"/app/1086940/\" tabindex=\"-1\" aria-hidden=\"true\">\n",
    "<img src=\"https://shared.cloudflare.steamstatic.com/store_item_assets/steam/apps/1086940/capsule_231x87.jpg?t=1730915928\" alt=\"\" decoding=\"async\">\n",
    "</a>\n",
    "</td>\n",
    "<td>\n",
    "<a target=\"_blank\" class=\"b\" href=\"/app/1086940/\">Baldur's Gate 3</a>\n",
    "<div class=\"subinfo\">\n",
    "<span class=\"cat cat-week-long-deal\">Week Long Deal</span><span class=\"cat cat-top-seller\">Top Seller</span>\n",
    "</div>\n",
    "</td>\n",
    "\n",
    "<td class=\"price-discount dt-type-numeric\" data-sort=\"20\">-20%</td>\n",
    "\n",
    "<td data-sort=\"15999\" class=\"dt-type-numeric\">R$ 159,99</td>\n",
    "\n",
    "<td data-sort=\"95.98\" class=\"dt-type-numeric\">95.98%</td>\n",
    "\n",
    "<td data-sort=\"1691020800\" class=\"dt-type-numeric\">Aug 2023</td>\n",
    "\n",
    "<td data-sort=\"1731348000\" class=\"timeago dt-type-numeric\" title=\"11 November 2024 at 18:00:00 UTC\n",
    "11 November 2024 at 15:00:00 GMT-3\">in 5 days</td>\n",
    "\n",
    "<td data-sort=\"1730744570\" class=\"timeago dt-type-numeric\" title=\"4 November 2024 at 18:22:50 UTC\n",
    "4 November 2024 at 15:22:50 GMT-3\">2 days ago</td>\n",
    "\n",
    "</tr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from CloudflareBypasser import CloudflareBypasser\n",
    "from DrissionPage import ChromiumPage, ChromiumOptions\n",
    "import random\n",
    "import logging\n",
    "\n",
    "# Configuração do logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def atualizar_cookies(url=\"https://steamdb.info/cloudflare\"):\n",
    "    logging.info(\"Iniciando atualização de cookies.\")\n",
    "    tentativas = 0\n",
    "    max_tentativas = 5\n",
    "    cookies_necessarios = {}\n",
    "\n",
    "    while tentativas < max_tentativas:\n",
    "        logging.info(f\"Tentativa {tentativas + 1} de {max_tentativas}.\")\n",
    "        options = ChromiumOptions()\n",
    "        options.incognito(True)\n",
    "        driver = ChromiumPage(options)\n",
    "        \n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(random.randint(5, 10))\n",
    "            cf_bypasser = CloudflareBypasser(driver)\n",
    "            cf_bypasser.click_verification_button()\n",
    "            time.sleep(3)\n",
    "\n",
    "            cookies = driver.cookies()\n",
    "            logging.info(f\"Cookies coletados: {cookies}\")\n",
    "            for cookie in cookies:\n",
    "                if cookie['name'] == 'cf_clearance':\n",
    "                    cookies_necessarios['cf_clearance'] = cookie['value']\n",
    "                elif cookie['name'] == '__cf_bm':\n",
    "                    cookies_necessarios['__cf_bm'] = cookie['value']\n",
    "\n",
    "            if 'cf_clearance' in cookies_necessarios and '__cf_bm' in cookies_necessarios:\n",
    "                logging.info(f\"Cookies necessários obtidos: {cookies_necessarios}\")\n",
    "                return cookies_necessarios\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Erro durante a tentativa de obter cookies: {e}\")\n",
    "\n",
    "        finally:\n",
    "            driver.close()\n",
    "\n",
    "        tentativas += 1\n",
    "        logging.warning(f\"Tentativa {tentativas}: Cookie cf_clearance não encontrado, tentando novamente...\")\n",
    "\n",
    "    logging.error(\"Não foi possível obter o cookie cf_clearance após 5 tentativas.\")\n",
    "    raise Exception(\"Não foi possível obter o cookie cf_clearance após 5 tentativas.\")\n",
    "\n",
    "def obter_dados_steam_sales():\n",
    "    url = \"https://steamdb.info/sales/\"\n",
    "    headers = {\n",
    "        \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "        \"accept-language\": \"pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "        \"cache-control\": \"max-age=0\",\n",
    "        \"priority\": \"u=0, i\",\n",
    "        \"sec-ch-ua\": \"\\\"Google Chrome\\\";v=\\\"129\\\", \\\"Not=A?Brand\\\";v=\\\"8\\\", \\\"Chromium\\\";v=\\\"129\\\"\",\n",
    "        \"sec-ch-ua-arch\": \"\\\"x86\\\"\",\n",
    "        \"sec-ch-ua-bitness\": \"\\\"64\\\"\",\n",
    "        \"sec-ch-ua-full-version\": \"\\\"129.0.6668.100\\\"\",\n",
    "        \"sec-ch-ua-full-version-list\": \"\\\"Google Chrome\\\";v=\\\"129.0.6668.100\\\", \\\"Not=A?Brand\\\";v=\\\"8.0.0.0\\\", \\\"Chromium\\\";v=\\\"129.0.6668.100\\\"\",\n",
    "        \"sec-ch-ua-mobile\": \"?0\",\n",
    "        \"sec-ch-ua-model\": \"\\\"\\\"\",\n",
    "        \"sec-ch-ua-platform\": \"\\\"Linux\\\"\",\n",
    "        \"sec-ch-ua-platform-version\": \"\\\"6.8.0\\\"\",\n",
    "        \"sec-fetch-dest\": \"document\",\n",
    "        \"sec-fetch-mode\": \"navigate\",\n",
    "        \"sec-fetch-site\": \"none\",\n",
    "        \"sec-fetch-user\": \"?1\",\n",
    "        \"upgrade-insecure-requests\": \"1\"\n",
    "    }\n",
    "\n",
    "    cookies = {\n",
    "        \"__cf_bm\": \"RkKMCysd_cx0fJRmLdo8VmBNRB.cSIu8IYOpVYBGS84-1731034677-1.0.1.1-YXZk94pZHGPvqppkhSSb2AfnJfZigL51GVKFP64jSZwjbilk2kbz8YFtwJgNE.WJ5kYYyjKT5.FLj_YdmlLgbg\",\n",
    "        \"cf_clearance\": \"HdReNjNt6WT58x1JQ2D0QzNfv.02hCN.S7on3ZdtOpM-1730951425-1.2.1.1-0NrL2GdIoPxDAo0EWghgFtvZARIX7oCchL2s7GodVavunLgjlpT85ITcY8WGkJ54f00s3Le5Ot2BQX3U.hCjU0nQFdJTbKf9lVTuxlysXZCioz8l0GY8LqRJxnV4VuiMhu0BA7l2xrSQGfgCTE722ugkaiLMILeNEsS2ZjgiF0LzX5ut6xcQmGNK7s6Of1YSbAmxRCzQF6rYC0CQP01rLuPWwhqsKR1ELXilL_LQ9fPP.yfu9LihHm8GD6_QwnP75_RrqTBmbe3VhnU2giWN.wM_mkzvNtXfsaQIGzbQAjFY9F06oiXA_Ht93SChltWBwRKurDeBY7PzbXsyWEBDZ2tcKh4M5GxXQPiqGjplyEQgEDhfjMC.6CweP0bZrF_ZrhCqgKucbXdaVSQLsBUMqaY7znxxhwYaS6TmQbmVgN0\"\n",
    "    }\n",
    "\n",
    "    logging.info(\"Iniciando obtenção de dados de vendas do Steam.\")\n",
    "    response = requests.get(url, headers=headers, cookies=cookies)\n",
    "    logging.info(f\"Status da requisição: {response.status_code}\")\n",
    "\n",
    "    if response.status_code == 403:\n",
    "        logging.warning(\"Erro 403: Atualizando cookies...\")\n",
    "        cookies = atualizar_cookies()\n",
    "        response = requests.get(url, headers=headers, cookies=cookies)\n",
    "        logging.info(f\"Status da nova requisição: {response.status_code}\")\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        logging.info(\"Requisição bem-sucedida!\")\n",
    "\n",
    "        # Parsing do HTML e extração dos dados\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        names, discounts, prices, ratings, releases, ends, starts, game_links, image_links, deals = [], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "        # Localiza as linhas de dados de vendas na tabela\n",
    "        table_rows = soup.select('tr.app')\n",
    "\n",
    "        for row in table_rows:\n",
    "            # Extrai dados relevantes de cada coluna\n",
    "            name = row.select_one('.b')\n",
    "            discount = row.select_one('.price-discount')\n",
    "            price = row.select('td')[4]\n",
    "            rating = row.select('td')[5]\n",
    "            release = row.select('td')[6]\n",
    "            end = row.select('td')[7]\n",
    "            start = row.select('td')[8]\n",
    "            game_link = row.select_one('.info-icon')['href']\n",
    "            image = row.select_one('img')\n",
    "            deal = row.select_one('.cat-week-long-deal')\n",
    "\n",
    "            names.append(name.text.strip() if name else '')\n",
    "            discounts.append(discount.text.strip() if discount else '')\n",
    "            prices.append(price.text.strip() if price else '')\n",
    "            ratings.append(rating.text.strip() if rating else '')\n",
    "            releases.append(release.text.strip() if release else '')\n",
    "            ends.append(end.text.strip() if end else '')\n",
    "            starts.append(start.text.strip() if start else '')\n",
    "            game_links.append(game_link if game_link else '')\n",
    "\n",
    "            # Processa o URL da imagem: remove timestamp, se presente\n",
    "            if image and image.get('src'):\n",
    "                img_url = image['src']\n",
    "                img_url_clean = re.sub(r'\\?.*', '', img_url)\n",
    "                image_links.append(img_url_clean)\n",
    "            else:\n",
    "                image_links.append('')\n",
    "\n",
    "            deals.append(deal.text.strip() if deal else '')\n",
    "\n",
    "        # Cria um DataFrame a partir dos dados extraídos\n",
    "        sales_data = pd.DataFrame({\n",
    "            'Name': names,\n",
    "            'Discount %': discounts,\n",
    "            'Price': prices,\n",
    "            'Rating': ratings,\n",
    "            'Release Date': releases,\n",
    "            'Ends': ends,\n",
    "            'Started': starts,\n",
    "            'Game Link': game_links,\n",
    "            'Image Link': image_links,\n",
    "            'Deal': deals\n",
    "        })\n",
    "\n",
    "        # Salva o DataFrame para CSV\n",
    "        output_path = 'steam_sales_data_test.csv'\n",
    "        sales_data.to_csv(output_path, index=False)\n",
    "        logging.info(\"Arquivo CSV salvo com sucesso!\")\n",
    "        return output_path\n",
    "    else:\n",
    "        logging.error(f\"Erro ao fazer requisição: {response.status_code}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    output_path = obter_dados_steam_sales()\n",
    "    logging.info(f\"Arquivo salvo em: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 02:52:53,160 - INFO - Iniciando obtenção de dados de vendas do Steam.\n",
      "2024-11-08 02:53:24,252 - INFO - Status da requisição: 200\n",
      "2024-11-08 02:53:24,254 - INFO - Requisição bem-sucedida!\n",
      "2024-11-08 02:53:29,107 - INFO - Arquivo CSV salvo com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from CloudflareBypasser import CloudflareBypasser\n",
    "from DrissionPage import ChromiumPage, ChromiumOptions\n",
    "import random\n",
    "import logging\n",
    "\n",
    "# Configuração do logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "SCRAPEDO_API_KEY = \"49b49be9b12e4ac0b4699a280ccfe37a48e1f129623\"\n",
    "\n",
    "#7ea0e0a48c9248a182db1636bd2c87adfc93c754751\n",
    "#3bc24569df8942b99f762dd8ebedd2b31d869d03339\n",
    "\n",
    "def atualizar_cookies(url=\"https://steamdb.info/cloudflare\"):\n",
    "    logging.info(\"Iniciando atualização de cookies.\")\n",
    "    tentativas = 0\n",
    "    max_tentativas = 5\n",
    "    cookies_necessarios = {}\n",
    "\n",
    "    while tentativas < max_tentativas:\n",
    "        logging.info(f\"Tentativa {tentativas + 1} de {max_tentativas}.\")\n",
    "        options = ChromiumOptions()\n",
    "        options.incognito(True)\n",
    "        driver = ChromiumPage(options)\n",
    "        \n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(random.randint(5, 10))\n",
    "            cf_bypasser = CloudflareBypasser(driver)\n",
    "            cf_bypasser.click_verification_button()\n",
    "            time.sleep(3)\n",
    "\n",
    "            cookies = driver.cookies()\n",
    "            logging.info(f\"Cookies coletados: {cookies}\")\n",
    "            for cookie in cookies:\n",
    "                if cookie['name'] == 'cf_clearance':\n",
    "                    cookies_necessarios['cf_clearance'] = cookie['value']\n",
    "                elif cookie['name'] == '__cf_bm':\n",
    "                    cookies_necessarios['__cf_bm'] = cookie['value']\n",
    "\n",
    "            if 'cf_clearance' in cookies_necessarios and '__cf_bm' in cookies_necessarios:\n",
    "                logging.info(f\"Cookies necessários obtidos: {cookies_necessarios}\")\n",
    "                return cookies_necessarios\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Erro durante a tentativa de obter cookies: {e}\")\n",
    "\n",
    "        finally:\n",
    "            driver.close()\n",
    "\n",
    "        tentativas += 1\n",
    "        logging.warning(f\"Tentativa {tentativas}: Cookie cf_clearance não encontrado, tentando novamente...\")\n",
    "\n",
    "    logging.error(\"Não foi possível obter o cookie cf_clearance após 5 tentativas.\")\n",
    "    raise Exception(\"Não foi possível obter o cookie cf_clearance após 5 tentativas.\")\n",
    "\n",
    "def obter_dados_steam_sales():\n",
    "    url = \"https://steamdb.info/sales/\"\n",
    "    headers = {\n",
    "        \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "        \"accept-language\": \"pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "        \"cache-control\": \"max-age=0\",\n",
    "        \"priority\": \"u=0, i\",\n",
    "        \"sec-ch-ua\": \"\\\"Google Chrome\\\";v=\\\"129\\\", \\\"Not=A?Brand\\\";v=\\\"8\\\", \\\"Chromium\\\";v=\\\"129\\\"\",\n",
    "        \"sec-ch-ua-arch\": \"\\\"x86\\\"\",\n",
    "        \"sec-ch-ua-bitness\": \"\\\"64\\\"\",\n",
    "        \"sec-ch-ua-full-version\": \"\\\"129.0.6668.100\\\"\",\n",
    "        \"sec-ch-ua-full-version-list\": \"\\\"Google Chrome\\\";v=\\\"129.0.6668.100\\\", \\\"Not=A?Brand\\\";v=\\\"8.0.0.0\\\", \\\"Chromium\\\";v=\\\"129.0.6668.100\\\"\",\n",
    "        \"sec-ch-ua-mobile\": \"?0\",\n",
    "        \"sec-ch-ua-model\": \"\\\"\\\"\",\n",
    "        \"sec-ch-ua-platform\": \"\\\"Linux\\\"\",\n",
    "        \"sec-ch-ua-platform-version\": \"\\\"6.8.0\\\"\",\n",
    "        \"sec-fetch-dest\": \"document\",\n",
    "        \"sec-fetch-mode\": \"navigate\",\n",
    "        \"sec-fetch-site\": \"none\",\n",
    "        \"sec-fetch-user\": \"?1\",\n",
    "        \"upgrade-insecure-requests\": \"1\"\n",
    "    }\n",
    "\n",
    "    cookies = {\n",
    "        \"__cf_bm\": \"RkKMCysd_cx0fJRmLdo8VmBNRB.cSIu8IYOpVYBGS84-1731034677-1.0.1.1-YXZk94pZHGPvqppkhSSb2AfnJfZigL51GVKFP64jSZwjbilk2kbz8YFtwJgNE.WJ5kYYyjKT5.FLj_YdmlLgbg\",\n",
    "        \"cf_clearance\": \"HdReNjNt6WT58x1JQ2D0QzNfv.02hCN.S7on3ZdtOpM-1730951425-1.2.1.1-0NrL2GdIoPxDAo0EWghgFtvZARIX7oCchL2s7GodVavunLgjlpT85ITcY8WGkJ54f00s3Le5Ot2BQX3U.hCjU0nQFdJTbKf9lVTuxlysXZCioz8l0GY8LqRJxnV4VuiMhu0BA7l2xrSQGfgCTE722ugkaiLMILeNEsS2ZjgiF0LzX5ut6xcQmGNK7s6Of1YSbAmxRCzQF6rYC0CQP01rLuPWwhqsKR1ELXilL_LQ9fPP.yfu9LihHm8GD6_QwnP75_RrqTBmbe3VhnU2giWN.wM_mkzvNtXfsaQIGzbQAjFY9F06oiXA_Ht93SChltWBwRKurDeBY7PzbXsyWEBDZ2tcKh4M5GxXQPiqGjplyEQgEDhfjMC.6CweP0bZrF_ZrhCqgKucbXdaVSQLsBUMqaY7znxxhwYaS6TmQbmVgN0\"\n",
    "    }\n",
    "\n",
    "    logging.info(\"Iniciando obtenção de dados de vendas do Steam.\")\n",
    "\n",
    "    # Requisição através do Scrape.do\n",
    "    scrape_do_url = f\"https://api.scrape.do?token={SCRAPEDO_API_KEY}&url={url}\"\n",
    "    response = requests.get(scrape_do_url, headers=headers, cookies=cookies)\n",
    "    logging.info(f\"Status da requisição: {response.status_code}\")\n",
    "\n",
    "    if response.status_code == 403:\n",
    "        logging.warning(\"Erro 403: Atualizando cookies...\")\n",
    "        cookies = atualizar_cookies()\n",
    "        response = requests.get(scrape_do_url, headers=headers, cookies=cookies)\n",
    "        logging.info(f\"Status da nova requisição: {response.status_code}\")\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        logging.info(\"Requisição bem-sucedida!\")\n",
    "\n",
    "        # Parsing do HTML e extração dos dados\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        names, discounts, prices, ratings, releases, ends, starts, game_links, image_links, ids = [], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "        # Localiza as linhas de dados de vendas na tabela\n",
    "        table_rows = soup.select('tr.app')\n",
    "\n",
    "        for row in table_rows:\n",
    "            name = row.select_one('.b')\n",
    "            discount = row.select_one('.price-discount')\n",
    "            price = row.select('td')[4]\n",
    "            rating = row.select('td')[5]\n",
    "            release = row.select('td')[6]\n",
    "            end = row.select('td')[7]\n",
    "            start = row.select('td')[8]\n",
    "            game_link = row.select_one('.info-icon')['href']\n",
    "            image = row.select_one('img')\n",
    "            regex = r\"\\/app\\/(\\d+)\"\n",
    "            id = (re.search(regex, game_link)).group(1)\n",
    "\n",
    "\n",
    "            names.append(name.text.strip() if name else '')\n",
    "            discounts.append(discount.text.strip() if discount else '')\n",
    "            prices.append(price.text.strip() if price else '')\n",
    "            ratings.append(rating.text.strip() if rating else '')\n",
    "            releases.append(release.text.strip() if release else '')\n",
    "            ends.append(end.text.strip() if end else '')\n",
    "            starts.append(start.text.strip() if start else '')\n",
    "            game_links.append(game_link if game_link else '')\n",
    "            ids.append(id)\n",
    "\n",
    "            if image and image.get('src'):\n",
    "                img_url = image['src']\n",
    "                img_url_clean = re.sub(r'\\?.*', '', img_url)\n",
    "                image_links.append(img_url_clean)\n",
    "            else:\n",
    "                image_links.append('')\n",
    "\n",
    "        sales_data = pd.DataFrame({\n",
    "            'Name': names,\n",
    "            'Discount %': discounts,\n",
    "            'Price': prices,\n",
    "            'Rating': ratings,\n",
    "            'Release Date': releases,\n",
    "            'Ends': ends,\n",
    "            'Starts': starts,\n",
    "            'Game Link': game_links,\n",
    "            'Image Link': image_links,\n",
    "            'ID': ids\n",
    "        })\n",
    "\n",
    "        return sales_data\n",
    "    else:\n",
    "        logging.error(\"Erro na obtenção de dados. Código de status:\", response.status_code)\n",
    "        return None\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    data = obter_dados_steam_sales()\n",
    "    if data is not None:\n",
    "        output_path = 'steam_sales_data_test.csv'\n",
    "        data.to_csv(output_path, index=False)\n",
    "        logging.info(\"Arquivo CSV salvo com sucesso!\")\n",
    "    else:\n",
    "        print(\"Erro ao obter dados de vendas do Steam.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('steam_sales_data_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Name': 'Insaniquarium Deluxe',\n",
       " 'Discount %': '-80%',\n",
       " 'Price': '$0.99',\n",
       " 'Rating': '93.36%',\n",
       " 'Release Date': 'Aug 2006',\n",
       " 'Ends': '',\n",
       " 'Starts': '',\n",
       " 'Game Link': 'https://store.steampowered.com/app/3320/?curator_clanid=4777282&utm_source=SteamDB',\n",
       " 'Image Link': '/static/img/applogo.svg',\n",
       " 'ID': '3320'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[1].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 03:13:25,585 - INFO - Iniciando obtenção de dados de vendas do Steam.\n",
      "2024-11-08 03:13:43,204 - INFO - Status da requisição: 200\n",
      "2024-11-08 03:13:43,205 - INFO - Requisição bem-sucedida!\n"
     ]
    },
    {
     "ename": "ConnectTimeout",
     "evalue": "HTTPSConnectionPool(host='api.scrape.do', port=443): Max retries exceeded with url: /?token=7ea0e0a48c9248a182db1636bd2c87adfc93c754751&url=https://steamdb.info/api/RenderAppHover/?appid=3390 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7cabdae89ae0>, 'Connection to api.scrape.do timed out. (connect timeout=None)'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/util/connection.py:95\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetaddrinfo returns an empty list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 85\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sock\n",
      "\u001b[0;31mTimeoutError\u001b[0m: [Errno 110] Connection timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectTimeoutError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:404\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 404\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1060\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1060\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connection.py:363\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;66;03m# Add certificate verification\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m     hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connection.py:179\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m timed out. (connect timeout=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout),\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mConnectTimeoutError\u001b[0m: (<urllib3.connection.HTTPSConnection object at 0x7cabdae89ae0>, 'Connection to api.scrape.do timed out. (connect timeout=None)')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:801\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    799\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m--> 801\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/util/retry.py:594\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 594\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    596\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='api.scrape.do', port=443): Max retries exceeded with url: /?token=7ea0e0a48c9248a182db1636bd2c87adfc93c754751&url=https://steamdb.info/api/RenderAppHover/?appid=3390 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7cabdae89ae0>, 'Connection to api.scrape.do timed out. (connect timeout=None)'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectTimeout\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 202\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 202\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mobter_dados_steam_sales\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m         output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteam_sales_data_test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[28], line 191\u001b[0m, in \u001b[0;36mobter_dados_steam_sales\u001b[0;34m()\u001b[0m\n\u001b[1;32m    189\u001b[0m additional_info \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m app_id \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[0;32m--> 191\u001b[0m     additional_info\u001b[38;5;241m.\u001b[39mappend(\u001b[43mfetch_additional_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp_id\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    193\u001b[0m additional_info_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(additional_info)\n\u001b[1;32m    194\u001b[0m sales_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([sales_data, additional_info_df], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[28], line 170\u001b[0m, in \u001b[0;36mobter_dados_steam_sales.<locals>.fetch_additional_info\u001b[0;34m(app_id)\u001b[0m\n\u001b[1;32m    166\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://steamdb.info/api/RenderAppHover/?appid=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapp_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m scrape_do_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.scrape.do?token=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mget_next_api_key()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&url=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 170\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscrape_do_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcookies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcookies\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m    172\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/adapters.py:507\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, NewConnectionError):\n\u001b[0;32m--> 507\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ResponseError):\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RetryError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectTimeout\u001b[0m: HTTPSConnectionPool(host='api.scrape.do', port=443): Max retries exceeded with url: /?token=7ea0e0a48c9248a182db1636bd2c87adfc93c754751&url=https://steamdb.info/api/RenderAppHover/?appid=3390 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7cabdae89ae0>, 'Connection to api.scrape.do timed out. (connect timeout=None)'))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from CloudflareBypasser import CloudflareBypasser\n",
    "from DrissionPage import ChromiumPage, ChromiumOptions\n",
    "import random\n",
    "import logging\n",
    "\n",
    "# Configuração do logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "API_KEYS = [\n",
    "    \"49b49be9b12e4ac0b4699a280ccfe37a48e1f129623\",\n",
    "    \"7ea0e0a48c9248a182db1636bd2c87adfc93c754751\",\n",
    "    \"3bc24569df8942b99f762dd8ebedd2b31d869d03339\"\n",
    "]\n",
    "api_key_index = 0\n",
    "\n",
    "def get_next_api_key():\n",
    "    global api_key_index\n",
    "    api_key = API_KEYS[api_key_index]\n",
    "    api_key_index = (api_key_index + 1) % len(API_KEYS)\n",
    "    return api_key\n",
    "\n",
    "def atualizar_cookies(url=\"https://steamdb.info/cloudflare\"):\n",
    "    logging.info(\"Iniciando atualização de cookies.\")\n",
    "    tentativas = 0\n",
    "    max_tentativas = 5\n",
    "    cookies_necessarios = {}\n",
    "\n",
    "    while tentativas < max_tentativas:\n",
    "        logging.info(f\"Tentativa {tentativas + 1} de {max_tentativas}.\")\n",
    "        options = ChromiumOptions()\n",
    "        options.incognito(True)\n",
    "        driver = ChromiumPage(options)\n",
    "        \n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(random.randint(5, 10))\n",
    "            cf_bypasser = CloudflareBypasser(driver)\n",
    "            cf_bypasser.click_verification_button()\n",
    "            time.sleep(3)\n",
    "\n",
    "            cookies = driver.cookies()\n",
    "            logging.info(f\"Cookies coletados: {cookies}\")\n",
    "            for cookie in cookies:\n",
    "                if cookie['name'] == 'cf_clearance':\n",
    "                    cookies_necessarios['cf_clearance'] = cookie['value']\n",
    "                elif cookie['name'] == '__cf_bm':\n",
    "                    cookies_necessarios['__cf_bm'] = cookie['value']\n",
    "\n",
    "            if 'cf_clearance' in cookies_necessarios and '__cf_bm' in cookies_necessarios:\n",
    "                logging.info(f\"Cookies necessários obtidos: {cookies_necessarios}\")\n",
    "                return cookies_necessarios\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Erro durante a tentativa de obter cookies: {e}\")\n",
    "\n",
    "        finally:\n",
    "            driver.close()\n",
    "\n",
    "        tentativas += 1\n",
    "        logging.warning(f\"Tentativa {tentativas}: Cookie cf_clearance não encontrado, tentando novamente...\")\n",
    "\n",
    "    logging.error(\"Não foi possível obter o cookie cf_clearance após 5 tentativas.\")\n",
    "    raise Exception(\"Não foi possível obter o cookie cf_clearance após 5 tentativas.\")\n",
    "\n",
    "def obter_dados_steam_sales():\n",
    "    url = \"https://steamdb.info/sales/\"\n",
    "    headers = {\n",
    "        \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "        \"accept-language\": \"pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "        \"cache-control\": \"max-age=0\",\n",
    "        \"priority\": \"u=0, i\",\n",
    "        \"sec-ch-ua\": \"\\\"Google Chrome\\\";v=\\\"129\\\", \\\"Not=A?Brand\\\";v=\\\"8\\\", \\\"Chromium\\\";v=\\\"129\\\"\",\n",
    "        \"sec-ch-ua-arch\": \"\\\"x86\\\"\",\n",
    "        \"sec-ch-ua-bitness\": \"\\\"64\\\"\",\n",
    "        \"sec-ch-ua-full-version\": \"\\\"129.0.6668.100\\\"\",\n",
    "        \"sec-ch-ua-full-version-list\": \"\\\"Google Chrome\\\";v=\\\"129.0.6668.100\\\", \\\"Not=A?Brand\\\";v=\\\"8.0.0.0\\\", \\\"Chromium\\\";v=\\\"129.0.6668.100\\\"\",\n",
    "        \"sec-ch-ua-mobile\": \"?0\",\n",
    "        \"sec-ch-ua-model\": \"\\\"\\\"\",\n",
    "        \"sec-ch-ua-platform\": \"\\\"Linux\\\"\",\n",
    "        \"sec-ch-ua-platform-version\": \"\\\"6.8.0\\\"\",\n",
    "        \"sec-fetch-dest\": \"document\",\n",
    "        \"sec-fetch-mode\": \"navigate\",\n",
    "        \"sec-fetch-site\": \"none\",\n",
    "        \"sec-fetch-user\": \"?1\",\n",
    "        \"upgrade-insecure-requests\": \"1\"\n",
    "    }\n",
    "\n",
    "    cookies = {\n",
    "        \"__cf_bm\": \"RkKMCysd_cx0fJRmLdo8VmBNRB.cSIu8IYOpVYBGS84-1731034677-1.0.1.1-YXZk94pZHGPvqppkhSSb2AfnJfZigL51GVKFP64jSZwjbilk2kbz8YFtwJgNE.WJ5kYYyjKT5.FLj_YdmlLgbg\",\n",
    "        \"cf_clearance\": \"HdReNjNt6WT58x1JQ2D0QzNfv.02hCN.S7on3ZdtOpM-1730951425-1.2.1.1-0NrL2GdIoPxDAo0EWghgFtvZARIX7oCchL2s7GodVavunLgjlpT85ITcY8WGkJ54f00s3Le5Ot2BQX3U.hCjU0nQFdJTbKf9lVTuxlysXZCioz8l0GY8LqRJxnV4VuiMhu0BA7l2xrSQGfgCTE722ugkaiLMILeNEsS2ZjgiF0LzX5ut6xcQmGNK7s6Of1YSbAmxRCzQF6rYC0CQP01rLuPWwhqsKR1ELXilL_LQ9fPP.yfu9LihHm8GD6_QwnP75_RrqTBmbe3VhnU2giWN.wM_mkzvNtXfsaQIGzbQAjFY9F06oiXA_Ht93SChltWBwRKurDeBY7PzbXsyWEBDZ2tcKh4M5GxXQPiqGjplyEQgEDhfjMC.6CweP0bZrF_ZrhCqgKucbXdaVSQLsBUMqaY7znxxhwYaS6TmQbmVgN0\"\n",
    "    }\n",
    "\n",
    "    logging.info(\"Iniciando obtenção de dados de vendas do Steam.\")\n",
    "\n",
    "    # Requisição através do Scrape.do\n",
    "    scrape_do_url = f\"https://api.scrape.do?token={get_next_api_key()}&url={url}\"\n",
    "    response = requests.get(scrape_do_url, headers=headers, cookies=cookies)\n",
    "    logging.info(f\"Status da requisição: {response.status_code}\")\n",
    "\n",
    "    if response.status_code == 403:\n",
    "        logging.warning(\"Erro 403: Atualizando cookies...\")\n",
    "        cookies = atualizar_cookies()\n",
    "        response = requests.get(scrape_do_url, headers=headers, cookies=cookies)\n",
    "        logging.info(f\"Status da nova requisição: {response.status_code}\")\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        logging.info(\"Requisição bem-sucedida!\")\n",
    "\n",
    "        # Parsing do HTML e extração dos dados\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        names, discounts, prices, ratings, releases, ends, starts, game_links, image_links, ids = [], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "        # Localiza as linhas de dados de vendas na tabela\n",
    "        table_rows = soup.select('tr.app')\n",
    "\n",
    "        for row in table_rows:\n",
    "            name = row.select_one('.b')\n",
    "            discount = row.select_one('.price-discount')\n",
    "            price = row.select('td')[4]\n",
    "            rating = row.select('td')[5]\n",
    "            release = row.select('td')[6]\n",
    "            end = row.select('td')[7]\n",
    "            start = row.select('td')[8]\n",
    "            game_link = row.select_one('.info-icon')['href']\n",
    "            image = row.select_one('img')\n",
    "            regex = r\"\\/app\\/(\\d+)\"\n",
    "            id = (re.search(regex, game_link)).group(1)\n",
    "\n",
    "\n",
    "            names.append(name.text.strip() if name else '')\n",
    "            discounts.append(discount.text.strip() if discount else '')\n",
    "            prices.append(price.text.strip() if price else '')\n",
    "            ratings.append(rating.text.strip() if rating else '')\n",
    "            releases.append(release.text.strip() if release else '')\n",
    "            ends.append(end.text.strip() if end else '')\n",
    "            starts.append(start.text.strip() if start else '')\n",
    "            game_links.append(game_link if game_link else '')\n",
    "            ids.append(id)\n",
    "\n",
    "            if image and image.get('src'):\n",
    "                img_url = image['src']\n",
    "                img_url_clean = re.sub(r'\\?.*', '', img_url)\n",
    "                image_links.append(img_url_clean)\n",
    "            else:\n",
    "                image_links.append('')\n",
    "\n",
    "        sales_data = pd.DataFrame({\n",
    "            'Name': names,\n",
    "            'Discount %': discounts,\n",
    "            'Price': prices,\n",
    "            'Rating': ratings,\n",
    "            'Release Date': releases,\n",
    "            'Ends': ends,\n",
    "            'Starts': starts,\n",
    "            'Game Link': game_links,\n",
    "            'Image Link': image_links,\n",
    "            'ID': ids\n",
    "        })\n",
    "\n",
    "        def fetch_additional_info(app_id):\n",
    "            url = f\"https://steamdb.info/api/RenderAppHover/?appid={app_id}\"\n",
    "\n",
    "            scrape_do_url = f\"https://api.scrape.do?token={get_next_api_key()}&url={url}\"\n",
    "            \n",
    "            response = requests.get(scrape_do_url, headers=headers, cookies=cookies)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                developer = soup.select_one(\".hover_body.hover_meta a.b\").text if soup.select_one(\".hover_body.hover_meta a.b\") else ''\n",
    "                release_date = soup.select(\".hover_body.hover_meta b\")[0].text if soup.select(\".hover_body.hover_meta b\") else ''\n",
    "                player_peak = soup.select(\".hover_body.hover_meta b\")[1].text if len(soup.select(\".hover_body.hover_meta b\")) > 1 else ''\n",
    "                followers = soup.select(\".hover_body.hover_meta b\")[2].text if len(soup.select(\".hover_body.hover_meta b\")) > 2 else ''\n",
    "                \n",
    "                return {\n",
    "                    \"Developer\": developer,\n",
    "                    \"Release Date\": release_date,\n",
    "                    \"24h Player Peak\": player_peak,\n",
    "                    \"Followers\": followers\n",
    "                }\n",
    "            else:\n",
    "                print(f\"Failed to fetch data for appid {app_id}\")\n",
    "                return None\n",
    "        \n",
    "        additional_info = []\n",
    "        for app_id in ids:\n",
    "            additional_info.append(fetch_additional_info(app_id))\n",
    "        \n",
    "        additional_info_df = pd.DataFrame(additional_info)\n",
    "        sales_data = pd.concat([sales_data, additional_info_df], axis=1)\n",
    "\n",
    "        return sales_data\n",
    "    else:\n",
    "        logging.error(\"Erro na obtenção de dados. Código de status:\", response.status_code)\n",
    "        return None\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    data = obter_dados_steam_sales()\n",
    "    if data is not None:\n",
    "        output_path = 'steam_sales_data_test.csv'\n",
    "        data.to_csv(output_path, index=False)\n",
    "        logging.info(\"Arquivo CSV salvo com sucesso!\")\n",
    "    else:\n",
    "        print(\"Erro ao obter dados de vendas do Steam.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
